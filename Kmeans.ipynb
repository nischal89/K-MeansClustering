# python codes 
# Importing the required libraries
import numpy as np 
import pandas as pd 
import re
import nltk
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction import text
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from nltk.tokenize import RegexpTokenizer
from nltk.stem.snowball import SnowballStemmer
%matplotlib inline


# Importing the csv file
df = pd.read_csv("~/Documents/xxx.csv", encoding = 'utf-8',error_bad_lines=False)
df.head()

#data slicing by column name and locations

selected_data = pd.DataFrame([df.iloc[i, ]  for i in range(0, df.shape[0])
                         if bool(re.search('atea', str(df.loc[i,'domain']))) == True])

selected_data = selected_data.reset_index(drop=True) #reindexing (very important) 

#Viewing any column of the selected data

pd.set_option('display.max_colwidth', -1)
data = selected_data["content"]
data.head()

# removing stop words 

punc = ['.', ',', '"', "'", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}',"%"]
stop_words = text.ENGLISH_STOP_WORDS.union(punc)
desc = selected_data['content'].values
vectorizer = TfidfVectorizer(stop_words = stop_words)
X = vectorizer.fit_transform(desc)

#Count Vectoriser then tidf transformer

vectorizer2 = TfidfVectorizer(stop_words = stop_words, tokenizer = tokenize)
X2 = vectorizer2.fit_transform(desc)
word_features2 = vectorizer2.get_feature_names()
print(len(word_features2))
print(word_features2[:50])

vectorizer3 = TfidfVectorizer(stop_words = stop_words, tokenizer = tokenize)
X3 = vectorizer3.fit_transform(desc)
words = vectorizer3.get_feature_names()


#checking numbers of clusters possible using elbow method

from sklearn.cluster import KMeans
wcss = []
for i in range(1,11):
    kmeans = KMeans(n_clusters=i,init='k-means++',max_iter=300,n_init=10,random_state=0)
    kmeans.fit(X3)
    wcss.append(kmeans.inertia_)
plt.plot(range(1,11),wcss)
plt.title('The Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.savefig('elbow.png')
plt.show()

#Finally, the clusters

kmeans = KMeans(n_clusters = 4, n_init = 20, n_jobs = 1)
kmeans.fit(X3)
# We look at 4 the clusters generated by k-means.
common_words = kmeans.cluster_centers_.argsort()[:,-1:-26:-1]
for num, centroid in enumerate(common_words):
    print(str(num) + ' : ' + ', '.join(words[word] for word in centroid))
